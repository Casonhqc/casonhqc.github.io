---
date: 2024-09-05 23:05:56
modify: 2024-09-05 23:05:56
author: Cason
tags:
  - Redis
  - 读书笔记
aliases:
title: 《Redis开发与运维》读书笔记
categories:
  - 书籍笔记
share: true
---
# Redis 特性
## 1.速度快
- **Redis** **的数据都是放在内存中**，相比于 Mysql 这类从磁盘加载数据的数据库要快上几个数量级
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250820113214.png)

- **Redis** **是用 C 语言实现的**，一般来说 C 语言实现的程序更加底层，执行速度相对快
- **Redis** **使用单线程架构**，不需要考虑多线程产生的竞争问题
- 作为**开源项目**，作者自身的超高代码水平再结合世界各地的 Contributor 的贡献，造就了 Redis 的高性能。
## 2.基于键值对的数据结构服务器
Redis 键值对中的值不仅可以是字符串，还可以是具体的数据结构。Redis 提供了五大数据结构：字符串、哈希、列表、集合、有序集合。还在字符串的基础上演变出了其他的数据结构
## 3.丰富的功能
Redis 在提供键值对数据存储功能之外，还提供了许多其他的功能：
- 提供**键过期**
- 提供**发布订阅功能**，可以用来实现消息队列。
- 支持 lua 脚本功能，可以**原子性地，组合式地执行** **Redis** **命令**
- 提供简单的事务功能，**一定程度上保证事务特性**
- 提供 pipline 功能，客户端可以批量上传命令，减少网络开销
## 4.简单稳定
- Redis 的源码少，意味着容易吃透代码，更好地定位问题和优化。
- Redis 使用单线程模型，开发时不需要考虑线程安全问题
- Redis 内部使用了一个事件循环（event loop）来处理客户端请求、定时任务等事件。是 Redis 自己实现的，不需要外部类库的支持。
# API 的理解和使用
## 全局命令
- **查询所有键：** keys *
- **键总数**： dbsize
> `dbsize`是直接获取 Redis 内置的键总数，所以时间复杂度是 O（1），而`keys`命令需要遍历所有键，时间复杂度是 O（n）。当 Redis 保存了大量键时，线上环境禁止使用`keys`命令
- 键是否存在：exists key
- 删除键： del key
- 键过期：expire key seconds
- 键对应值的数据类型： type key
## 数据结构和内部编码
每种数据结构都有自己的底层内部编码实现，而且是多种实现，Redis 会在合适的场景选择合适的内部编码。
这样的设计有两个好处：
- 改进内部编码，对外的数据结构和命令没有影响。**比如 list 早先只有 linkedlist 和 ziplist，而后设计除了结合二者优势的 quicklist，但是用户没有感知**。
- 多种编码实现可以使用不同的场景。比如 ziplist 比较省空间，但是元素多的时候，性能下降，这时候就可以根据配置切换。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250820140131.png)
## 字符串
### 重要命令
1. Set key value 创建键值对，等于创建加更新。setnx 必须键不存在，才可以设置成功，代表只能添加。**setxx 必须键存在才能执行成功，代表只能更新**。
2. 可以用` mget key1 key2 key3 `来批量获取值，提高开发效率，也能够降低网络耗时。但是批量操作的数量不是无节制的，数量过多会导致 Redis 阻塞或者网络拥塞
3. incr 命令进行自增，**因为** **Redis** **单线程的架构，不需要考虑加锁**
### 内部编码
- int：8 个字节的长整型。
- embstr：小于等于 39 个字节的字符串。直接存储在Redis的对象结构中
- raw：大于 39 个字节的字符串。单独分配内存空间
### 使用场景
#### 缓存
Redis 没有命令空间，设计合理的键名可以组织键冲突，提高项目的可维护性。推荐使用“业务名： 对象名： id ： [属性]” 。如果键名较长，可以在能够描述含义的情况下减上长度，比如
```Java
user：{uid}：friends：messages：{mid}
u：{uid}：fr：m：{mid}
```
#### 计数
可以实现快速计数、查询缓存的功能。
#### 共享 Session

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250820140315.png)
#### 限速
利用 setnx 和递增 incr 来计数和频控。setnx + 过期时间避免一段时间内的反复请求，incr 保证计数的不会出错，限制总请求数
```Java
key = "shortMsg:limit:" + phoneNum;
// SET key value EX 60 NX
isExists = redis.set(key,1,60,"NX");
if(isExists != null || redis.incr(key) <=5){
    // 通过
}else{
    // 限速
```
## 哈希
Redis的Key对应的Value本身又是一个键值对。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250820142303.png)
### 重要命令
```Java
设置字段和值 
HSET key field value

获取指定字段的值 
HGET key field

获取所有字段和值 
HGETALL key

删除一个或多个字段 
HDEL key field1 [field2 ...]

同时设置多个字段和值 
HMSET key field1 value1 field2 value2 ..
```

可以使用hgetall key 获取所有的哈希元素，但是如果哈希元素个数较多，可能会阻塞。如果一定要获取全部，建议使用hscan命令。
### 内部编码
- ziplist：当哈希元素个数小于默认配置的512，且所有值都小于默认配置的64字节，使用ziplist更节省空间
- hashtable：当不满足ziplist条件时，使用hashTable（哈希表）实现。因为ziplist在大数量2情况下，读写效率下降
### 使用场景
##### 缓存
相比于使用字符串缓存信息，比如用户信息，会更加直观，并且在更新操作上更加方便。使用字符串必须要进行序列化过程。字符串的优点是更节省空间
## 列表
### 文章列表实现
```Shell
// 用hash存储文章多个属性
hmset article:1 title xx timestamp 1476536196 content xxx

//将hash的key存到队列里
lpush user:1:artcles article:1

//分页取出第一个用户的0到9篇文章
articles = lrange user:1:articles 0 9
//遍历每个hash类型的文章，获取所有的属性（键值对）
for article in {articles} 
    hgetall {article}
```
存在的问题：
1. 如果分页出来的文章数量过多，需要循环执行多次hgeall
    1. 考虑使用pipline
    2. 选择用字符串存文章数据，批量获取直接用mget
2. 如果列表较大，获取中间范围元素性能较低
    1. 考虑使用quicklist的内部编码
## 集合
集合和列表最大的不同是，集合不允许有重复的元素且无序。Redis中的集合支持进行集合运算，比如交集，并集，差集
### 重要命令
```Shell
获取元素个数，O(1), Redis内部使用一个变量维护
scard key

随机选出一个元素并删除
spop key
随机选出一个元素不删除
srandmemeber key

交集    并集    差集
sinter sunion sdiff
```
### 内部编码
可以通过`object encoding key`查看内部编码
- intset: 当集合内的元素都是整数类型， 且小于默认配置的512个。
- hashtable：只要不满足intset，直接使用hashtable。
### 标签功能

```Shell
// 给用户加上tag
sadd user:1:tag tage1 tag2 

// 给tag加上用户
sadd tag:1:users user:1 user2:2

//计算用户的共同tag（兴趣）
sinter user:1:tags user2:tags


计算用户的共同兴趣标签
sinter user:1:tags user:2:tags

计算一个兴趣下有多少用户
SMEMBERS tag:tag1:users
```
## 有序集合（sorted set）
既有集合不能重复元素的特性，又有列表的顺序特性。独特的特性是一个score，用来排序。
### 重要命令
有序集合也可以做集合运算[]()
```Shell
ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]
ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]

# 获取第一个zset的成员
ZRANGE zset1 0 -1 WITHSCORES

# 从第二个zset中移除这些成员
ZREM zset2 member1 member2 ...
```
- `destination`：将计算结果存到xxx键中。
- `numkeys`：需要做集合运算的key个数。
- `key`：要做集合运算的zset的键名。
- `WEIGHTS`：可选参数，指定每个zset的权重，默认为1。
- `AGGREGATE`：可选参数，指定如何聚合分数，可以是`SUM`（默认）、`MIN`或`MAX`
### 内部编码
- ziplist：当元素个数小于默认配置的128个，且元素的值大小小于默认配置的64字节。选择这种内部编码可以更节省空间
- skiplist：当ziplist条件不满足，即使用skiplist。
### 点赞功能

```Shell
点赞
zadd user:rank_likes:2024_08_05 mike 3
zincrby user:rank_likes:20204_08_05 mike 1

取消点赞
zrem user:rank_likes:2024_08_05 mike

展示点赞信息
zrevrangebyrank，zscore， zrank
```
## 管理键（Key）
### 单个键管理
#### 重命名Key
```Shell

rename key newkey 

// 只有在原先的key不存在的情况下才会被覆盖
renamenx key newkey
```
> -重命名Key实际是先del删除旧的Key，所以如果原先的Key过大，会阻塞Redis
>     
> - 如果key和newkey的值一样，在Redis3.2之前的版本会直接报错
>     

#### 控制Key的过期时间

```Shell
EXPIRE key seconds

TTL key

EXPIREAT key timestamp

移除键的过期时间，使其永不过期。
PERSIST key
```

**字符串的set命令会去除Key原先设置的过期时间**
#### 迁移Key
```Shell
在Redis内部的多个数据库之间进行迁移
move key db

//在一台机子上执行，备份数据
dump key

另一台机子复原数据
restore key ttl value

这个过程是开启了两个客户端连接，将序列化数据经过客户端进行传输
```
**migrate迁移**
`MIGRATE target_host target_port key destination_db timeout [COPY] [REPLACE]`
- `target_host`：目标 Redis 实例的主机名或 IP 地址。
- `target_port`：目标 Redis 实例的端口号。
- `key`：要迁移的键名。
- `destination_db`：目标 Redis 实例的数据库编号。
- `timeout`：超时时间（以毫秒为单位），表示迁移操作的最大执行时间。
- `COPY`：可选参数，表示迁移后不删除源实例中的键。
- `REPLACE`：可选参数，表示如果目标实例中已存在同名键，则替换它。
**实际上是将dump，restore，del三个命令进行组合**。首先这个过程是**原子性**的，不需要为每个Redis实例开启客户端，只需要在源Redis执行命令；其次这个备份的数据是直接在两个Redis之间进行转移的；最后目标Redis完成restore之后会发送Ok确认给源Redis，源Redis可以就此判断是否需要删除对应的键
### 渐进式遍历Key
如果直接使用Keys命令，在Key的数量较大时，会引发阻塞。scan命令类似于游标分页，执行`scan cursor`后，得到的返回结果包含下一个游标以及若干个Keys。**当遍历到最后一个元素时，返回的cursor就是0**
scan的这种思路也被用于解决其他阻塞问题。比如hgetall, smembers, zrange 等。在实际代码便携中，我们就可以使用scan配合while循环
### 数据库管理
Redis用16个数字区分多个数据，连接数据库默认到0号数据库。但是Redis逐渐弱化这个设计。在搭建Redis Cluster时，只允许使用0号数据库。总结有三点原因：
- Redis是单线程，使用多个数据库，**使用的仍然是一个****CPU**。彼此之间还有有影响
- **多数据库会让问题定位困难**：比如一个数据库的慢查询，实际是会影响到其他的数据库的。从宏观返回状态下，你很难发现当前数据库的性能问题，是来自其他数据库的慢查询
```Shell
    数据库0：用于存储用户会话信息（例如，登录状态）。
    数据库1：用于存储实时统计数据（例如，页面访问量）
    
    数据库1中有一个复杂的查询，例如一个大的KEYS命令或一个长时间运行的Lua脚本
    
    由于Redis实例的资源是共享的，这个慢查询会导致整个实例的性能下降，包括数据库0。
    数据库0中的业务方（例如，用户会话管理服务）会突然发现响应时间变长，甚至出现超时。
    业务方可能会误以为问题出在自己的代码或配置上，而实际上是由于数据库1中的慢查询导致的
    ```
- 部分Redis的客户端不支持切换数据库，即便支持，在开发的时候也容易弄混
# Redis的一些小工具

## 慢查询分析
Redis客户端到服务器执行一条命令分为**：发送命令，命令排队，命令执行，返回结果**。其中慢查询着重关注命令执行这个环节的时间。
### 两个配置参数
**slowlog-log-slower-than：** 如果执行命令时间超过了这个值，就会被记录在慢查询日志中。如果这个值设置小于0，那么不会记录任何命令；设置为0，则记录所有命令
**slowlog-max-len：**Redis使用内存中的一个列表来记录慢查询日志，所以如果新插入的命令，导致列表长度将突破这个值时，会将队头的数据移除后，再插入当前命令。
### 最佳实践
获取慢查询日志
`Slowlog get [n]`
每个慢查询日志结果由四部分组成：标识id，发生的时间戳，命令耗时，执行命令和对应的参数
```Shell
    slowlog get
 1) 1) (integer) 666
    2) (integer) 1456786500
    3) (integer) 11615
    4) 1) "BGREWRITEAOF"
    
    
 2) 1) (integer) 665
    2) (integer) 1456718400
    3) (integer) 12006
    4) 1) "SETEX"
       5) "video_info_200"
       6) "300"
       7) "2"
...
```
要点注意：
- **slowlog-max-len配置建议：线上建议调大慢查询列表，记录慢查询时Redis会对长命令做截断操作，并不会占用大量内存。增大慢查询列表可以减缓慢查询被剔除的可能**，例如线上可设置为1000以上。
- **slowlog-log-slower-than配置建议：默认值超过10毫秒判定为慢查询，需要根据Redis并发量调整该值。由于Redis采用单线程响应命令，对于高流量的场景，如果命令执行时间在1毫秒以上，那么Redis最多可支撑OPS不到1000**。因此对于高OPS场景的Redis建议设置为1毫秒。
- 慢查询只记录命令执行时间，并不包括命令排队和网络传输时间。因此**客户端执行命令的时间会大于命令实际执行时间**。因为命令执行排队机制，**慢查询会导致其他命令接连阻塞**，因此当客户端出现请求超时，需要检查该时间点是否有对应的慢查询，从而分析出是否为慢查询导致的命令级联阻塞。
- 由于慢查询日志是一个先进先出的队列，也就是说如果慢查询比较多的情况下，可能会丢失部分慢查询命令，为了防止这种情况发生，可以**定期执行slow get命令将慢查询日志持久化到其他存储中**（例如MySQL），然后可以制作可视化界面进行查询
## Pipeline
Redis的瓶颈主要是网络的延迟和吞吐，所以要**尽可能的减少网络的****IO****次数**，也就是使用批处理命令。Redis大多数的命令本身不是批量的，比如hgetall，获取一个hash的所有键值对，但是就没有mhgetall，需要执行n词。

pipeline的作用就是将一组Redis命令组装，然后通过一次命令的传输和执行完成，减少了网络往返的次数。目前很多编程语言客户端都支持pipeline，比如jedis

```Shell
        // 创建 Jedis 实例
        Jedis jedis = new Jedis("localhost", 6379);

        try {
            // 创建 Pipeline 实例
            Pipeline pipeline = jedis.pipelined();

            // 添加命令到 Pipeline
            pipeline.set("key1", "value1");
            pipeline.set("key2", "value2");
            pipeline.get("key1");
            pipeline.get("key2");

            // 执行 Pipeline
            List<Object> responses = pipeline.syncAndReturnAll();

            // 处理响应
            for (Object response : responses) {
                System.out.println("Response: " + response);
            }
        } finally {
            // 关闭 Jedis 连接
            jedis.close();
        }
```

通过下面这张表格的时间对比，得出
- 本机网络环境，性能提升约4.28倍
- 内网环境下，提升约6.71倍
- 在异地机房环境下，提升超过70倍
延迟时间越长，使用Pipeline的优势越明显

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822113422.png)

> Pipeline组装的命令个数不能没有节制，否则会增加客户端的等待时间，同时可能造成网络阻塞
## 事务
Redis提供简单的事务功能，用multi代表事务开始，用exec代表事务的结束。在事务中出现不同类型的错误，回滚机制不同
### 命令错误

也就是语法错误，比如把set写成了sett，不论执行的顺序，因为**事务开启后，都会进入Queued状态，这个错误会让整个事务无法执行。**
### 运行时错误
比如我用一个zadd命令去操作一个普通集合，会出现`WRONGTYPE Operation against a key holding the wrong kind of value`，但是这个错是在执行了命令之后抛出来的，所以zadd命令会失败，但是事务中的其他命令执行成功，无法回滚。
### Watch乐观锁
Redis是单线程执行命令，但是存在多个客户端，一个客户端事务内的多条命令不能保证原子性执行。为了确保事务中的key没有被其他的客户端修改，在事务开启前，添加一个watch命令，事务执行时，会做一个compare and set的乐观锁判断。
## lua脚本
一个lua脚本内的执行逻辑，redis命令，会被看作是一条命令来执行。
常用的lua脚本使用过程是：客户端调用script load 命令把脚本内容加载到Redis的内存中，然后得到一个SHA1的校验和。想要执行这个lua脚本，就通过下面的命令
```Shell
evalsha 脚本SHA1值 key个数 key列表参数列表
```

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822113617.png)

lua脚本会一直常驻在服务端，避免了发送Lua脚本的开销。脚本功能得到了复用。
  
lua脚本有三大好处：
- 首先就是前面说的**ua脚本是当作一条命令，原子执行的**
- 开发者可以利用lua脚本**个性化定制Redis命令，脚本常驻Redis内存**
- lua脚本可以将多条命令打包，**和pipeline一样会减少网络开销**
lua脚本执行的注意事项：
- lua脚本如果执行比较耗时，那么Redis会被阻塞，其他客户端的请求需要等到脚本执行结束或者进行外部干预
- 如果想终止lua脚本的执行，可以使用script kill。但是这个命令不能终止正在进行写操作的脚本
## 发布订阅
**发布者**（Publisher）发布消息到一个或多个频道（Channel），这个过程是非阻塞的。发布者无需等待订阅者（Subscriber）接收到消息后再继续执行其他操作。
`PUBLISH channel:sports "James lost the championship"`
**订阅者**（Subscriber）订阅一个或多个频道，订阅者会接收到发布者发布的消息。这些消息会通过 Redis 的事件循环机制异步推送给订阅者。
`SUBSCRIBE channel:sports`
注意：订阅者客户端在调用 `SUBSCRIBE` 命令后，会进入一个阻塞循环，持续等待来自服务器的消息。
# 客户端
## JAVA客户端大对比
**Jedis**:
- **同步支持**: Jedis 是一个同步的 Redis 客户端，所有命令都是阻塞的。
- **线程安全性**: Jedis 本身不是线程安全的，需要使用连接池（如 JedisPool）来管理多个线程的连接。[jedis线程安全分析](https://cloud.tencent.com/developer/article/1678172)
- **API** **风格**: API 设计接近原生 Redis 命令，简单易用。
- **适用场景**: 适合简单应用和小型项目，不需要复杂的异步处理和高级分布式功能。
**Lettuce**:
- **同步和异步支持**: 支持同步和异步操作，并且支持反应式编程（Reactive Programming）。
- **线程安全性**: 线程安全的多线程设计，适合高并发环境。
- **API** **风格**: 提供函数式编程风格的 API，支持复杂的异步和反应式操作。
- **适用场景**: 适合需要高并发处理、异步操作和反应式编程的应用。
**Redisson**:
- **同步和异步支持**: 支持同步和异步操作。
- **线程安全性**: 线程安全，自动管理连接池。
- **API** **风格**: 提供高级的面向对象 API，支持广泛的分布式对象和结构。
- **高级特性**: 提供分布式锁、分布式集合、分布式列表等高级分布式功能。
- **适用场景**: 适合复杂的分布式系统，需要高级分布式功能和对象支持的场景。
## 客户端管理命令
使用client list 可以列出所有客户端连接信息，包含以下信息
### 客户端标识
比如，自增的Id，addr包含客户端ip和端口，名字，socket文件描述符
### 输入缓冲区
Redis给每个客户端都会分配一块内存空间，**临时保存客户端的命令**，每次执行的命令都是从输入缓冲区拉取的。qbuf和qbuf-free就代表缓冲区的总容量和剩余容量。Redis要求客户端的缓冲区大小不能超过1G，否则客户端会被关闭
## 客户端常见异常

### 无法从连接池获取到连接

客户端角度分析，JedisPool的jedis对象默认个数是8个，获取不到连接那说明此时连接池中没有多余的Jedis对象了。有可能是高并发下，**资源都被抢走了**；也有可能是其他的调用者占用资源迟迟未归还，**这种未归还可能是存在慢查询，或者就是代码忘了写归还操作**。
从服务端角度分析，服务端可能是出于某些原因发生了阻塞，导致客户端迟迟无法归还连接。
### 客户端缓冲区异常
客户端输出缓冲区是用来**存放服务端执行命令后的结果的**，所以如果客户端请求的是一个大key，那么可能把输出缓冲区冲满。
还有一种可能性就是**Jedis对象在并发情况下操作**，可以回到上面的jedis并发问题分析。

### 客户端案例分析

#### Redis内存陡增

现象是服务端的主节点内存陡增，逼近maxmemory，而从节点没有变化。客户端抛出OOM异常，无法写入新的数据了

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822161845.png)

分析思路是：
- 主节点内存陡增，首先猜测有大量的写入。但是为什么从节点没有同步更新呢？最先想到的就是**同步出现异常**，通过dbsize查询两个节点的Key数量，发现一致。这个原因pass
- 还有其他的原因会导致主节点内存陡增吗？节点的内存除了存主要数据，**还有一部分是分出来作为输入和输出的缓冲区的**。通过info clients查询客户端的信息
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822161913.png)

发现这里输出缓冲区明显异常（最大的客户端输出缓冲区有20w+对象），一定有客户端的omem不是0（因为正常情况下，处理速度足够快，omem为0）。 用grep命令来定位，找到异常客户端的命令记录。发现是执行了monitor命令。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822162117.png)
#### 客户端周期性超时

客户端发现出现大量超时且周期性出现，而服务端没有明显的异常，有一些慢查询操作。
分析思路：
- 首先查看网络状况，观察网络是否有波动
- 观察Redis的日志，判断Redis本身是否出现异常
- 对比慢查询日志和客户端耗时监控，发现**周期相似**，定位到是慢查询操作导致的。

> 这个案例凸显出来了客户端监控的重要性。
# 持久化

Redis对数据的操作都是在内存完成的，我们需要持久化技术，保证在Redis进程退出，重启，宕机情况下，能够再次恢复到之前的状态。
## RDB
将某一个时刻下，数据库的所有数据保存为一个二进制文件。
### 触发机制
手动执行命令save或者bgsave会开始创建RDB快照文件，前者是同步阻塞，后者是通过fork异步完成的。
还有一些情况下是自动触发RDB持久化
1. 配置文件配置“save m n” 表示**m秒内如果数据集存在n次修改**，会自动触发bgsave
2. 主从架构下，从节点进行全量复制，**主节点会自动执行bgsave**生成RDB文件给从节点
3. 执行debug reload，重新加载Redis，也会触发
4. 在执行shutdown之后，也会自动触发
### 流程说明
1. 执行bgsave，先判断是否有**正在执行的RDB或AOF的子进程**，如果有直接返回
2. 父进程执行**fork操作**创建子进程，fork过程父进程处于阻塞状态。
3. fork完成后，bgsave返回结果，父进程继续响应其他命令
4. 子进程和父进程**共享同一块内存**，根据内存里的数据，生成临时快照文件，完成后覆盖原有的文件。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822162255.png)

这里的RDB文件存放到dir配置的指定目录下。还可以执行`config set dir{newDir}`来更改存放目录。
### 优缺点分析

#### 优点

- RDB是代表Redis在某一个时刻下的数据快照，即全部的数据，适用于备份，全量复制。
- RDB是二进制文件，恢复数据更快
#### 缺点
- RDB只能记录某一个时刻的数据，**时效性较低**
- RDB的二进制格式是特定的，存在一个版本的兼容性问题
- 因为bgsave会fork创建子进程，如果频繁执行，**成本太高**。
> - 内存复制和CPU开销
> - 磁盘I/O负载和空间占用 
> - 内核态和用户态的切换
## AOF
Append Only File, 只追加文件。做增量数据的备份。保存的是Redis执行过的写命令
### 流程说明
- 对于写入的命令，追加到AOF缓冲区中
- AOF缓冲区根据同步策略，选择时机执行sync命令，将数据同步到硬盘
- 当AOF文件过大时，会执行重写操作
- 当Redis重启，会加载AOF文件进行数据恢复
> 这里使用aof缓冲区是因为，Redis单线程响应命令，**如果每次写命令都直接写到硬盘，那么根据短板效应，性能就取决于磁盘负载了**。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822163839.png)


#### 同步策略
- always：每次写入AOF缓冲区都要同步到AOF文件。
- no：将同步的时机交给操作系统
- everysec：每秒进行一次同步，最多会有一秒的数据丢失

  ![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822163856.png)

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822163905.png)

### 重写机制
AOF是追加操作，文件的大小会一直增大，当超过一定大小，我们应该考虑缩减大小。
**重写是如何实现文件的缩小的？**
- 对已经过期的数据不再写入
- 对无效的命令，比如del，连续set等，都**只保留最终数据对应的写命令**
- 将多条写命令合成一个，比如**一次性将多个元素lpush到list中**
#### 流程分析
- 首先会判断是否有bgsave在执行，因为**重写的参考数据来自于RDB文件**，所以如果有bgsave，会等到bgsave结束后在执行。
- 父进程进行fork操作，阻塞直到fork完成，然后继续响应其他命令。期间的执行的写命令都会放到AOF重写缓冲区中。
- 子进程**根据RDB文件开始写入写命令**到新的AOF文件
- 新AOF文件写完成后，子进程发送信号给父进程，父进程将**AOF重写缓冲区的补充写入**到AOF文件中
- 用新的AOF文件替换老文件

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822163953.png)


## 问题定位和优化

### fork操作

#### fork操作潜在的问题是什么？

在进行RDB或者AOF重写时，都会执行fork操作。但是fork操作**相对耗时**，因为需要复制父进程的信息，其中虽然不需要直接拷贝父进程的内存空间，但是需要**复制内存页表**，在总内存较大时，也非常耗时间。其次还需要考虑到fork期间，**父进程一旦执行写操作，那么必然需要进行内存空间的复制**。
#### 如何优化
- 优先使用物理机，或者支持高效执行fork的虚拟化技术
- 合理控制每个Redis实例的内存
- **降低fork操作的频率**，对应就是尽量避免AOF的重写，RDB的频繁自动触发等。
### 子进程开销监控和优化

#### CPU
子进程的工作是把父进程的数据批量写入文件，这是一个CPU密集操作。所以应该避免出现CPU的过度竞争
- 比如不要把Redis绑定到单核CPU上，子进程会和父进程抢夺一个CPU
- 不要把Redis和其他的CPU密集型任务部署到一起。
#### 内存
子进程需要复制父进程的页表，如果父进程需要执行写入操作，那么还需要创建新的内存页。
所以需要**避免在大量写入时，做子进程重写操作**。
Linux后续更新了一个Transparent Huge Pages功能，这会导致fork的时候，每次复制的数据页单位从4KB变为2MB，大幅增加重写的内存消耗。
#### 硬盘
持久化的文件最后都是要写入磁盘的。磁盘写入也是有负载压力，所以优化的思路也是
- 不要和其他高硬盘负载服务部署在一台机子上，比如消息队列
- 普通机械硬盘的吞吐量一般在100MB/s，所以如果是高流量下的AOF，那么性能瓶颈就会出现在AOF同步磁盘
### AOF追加阻塞
AOF持久化，数据同步到磁盘的过程是由一个子线程执行fsync完成的。这个线程会在everysec配置下，每秒执行一次fsync，并且记录同步开始的时间。

主线程在执行下一次写命令，写入AOF缓冲区，就会去对比一下，现在和上次同步时间间隔多久：
- 如果在两秒内，那可以直接返回，放心让子线程完成最后同步。
- 如果大于两秒，那么会直接阻塞，直到同步操作完成。

这里选择阻塞，意味着这段时间内的请求都不能处理。也就不会有新的数据进入
  
# 复制

在分布式系统中，通常数据会有多份副本，部署在不同的机器上，满足故障恢复和负载均衡等需求。复制就是Redis高可用的基础，

## 拓扑图

### 一主一从

实现最基本的读写分离。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165301.png)

### 一主多从

从节点负责处理读命令，可以有效的降低慢查询对主节点的阻塞。但是多个从节点意味着，主节点需要多**次发送写命令给从节点来**保持数据的同步

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165313.png)


### 树状主从

部分从节点不仅复制主节点的数据，同时也可以作为其他从节点的主节点，将复制的数据向下继续传递。相当于将**主节点的负载和传输压力降低**，让二级主节点进一步承担。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165321.png)

## 流程分析

从节点执行slaveof命令后，经过六步完成复制。
### 1.保存主节点的信息
通过执行 info replication，就能够看到主节点的相关信息

```Shell
master_host:127.0.0.1
master_port:6379
master_link_status:down
```
### 2.与主节点建立socket命令
从节点要开启一个端口，建立socket来同步数据。这里是通过一个**定时任务**，每次任务执行，就是去查看是否发现新的主节点，然后尝试建立连接。这同时也说明，如果尝试一直失败，那么会一直重试
### 3.发送ping命令

**建立了连接之后，会发送ping**，看主从之间的socket是否可用，并且主节点当前是否能够接受且处理命令。如果收不到pong或者超时，就会**主动断开连接，等下次定时任务发起重连**
### 4.权限验证

考虑到有些主节点设置了密码验证，那么从节点需要提供相同的密码通过验证
### 5.同步数据

从节点通过发送**psync命令**给主节点，进行全量或者部分复制。

```Shell
命令格式：psync{runId}{offset}，
```

> - 这里的runId是Redis每次运行时产生的唯一ID
>     
> - offset代表从节点的复制进度（偏移量）
>     

在命令传播过程中，主从节点维护更新偏移量，这是二者数据一致性的体现。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165411.png)

#### 全量复制

当从节点和主节点第一次建立连接时，发起全量复制。这时候，从节点对主节点是很陌生的，所以psync命令的主**节点运行Id和复制偏移量都是-1.**

主节点收到这样的psync命令，明白这个从节点是第一次连接，所以会响应FULLRESYNC, 并且返回自己的运行ID和offset。

主节点还需要给从节点提供自己的数据集，所以要执行bgsave**，生成一份RDB文件**保存在本地，然后发送给从节点。

> 这里其实可以通过repl-diskless-sync参数，不落盘，直接通过网络传输过去。

从节点收到RDB文件后，也是保存到本地。

> 这里会有一种失败的情况。因为有一个配置叫repl-timeout，这是RDB文件传输耗时的阈值，如果文件过大，超过这个阈值，从节点会放弃复制的数据。

在从节点加载数据的时候，主节点也没有闲着，还是会处理写命令。为了同步这部分增量数据，主节点会将这些写命令保存在“复制客户端缓冲区”，在从节点完成RDB的加载后，将增量数据发送出去。
从节点加载RDB数据的过程，是首先要**把老数据给删除**了，然后再读取RDB文件。**如果这个时候有读命令，还是会响应。**
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165527.png)
#### 部分复制
就像全量复制，从节点由于在接受和加载RDB文件时，主节点还在正常响应命令，会存在数据不一致。那么后续也有可能因为网络，从节点的稳定性导致数据再次出现不一致。
所以主节点，还有一个“复制积压缓冲区”，大小默认1MB，可以**保存最近一段时间的写命令。**
如果主从的连接因为某些原因中断，那么当恢复之后，从节点中再次发起psync，而这时候，就会带上已知的参数“运行ID和offset”。
主节点验证运行ID和自己一致后，再检查offset是否还在缓冲区，如果在就会将offset后的数据返回。

### 6.保持连接持续复制数据

主从连接后，双方维持一个长链接，彼此发送心跳命令。
**主节点：**
每十秒发送ping，判断从节点的状态
**从节点：**
每秒发送replconf ack{offset} ，上报自己的offset，既可以监控和主节点的连接，又可以让主节点检查自己的数据是否一直，没有丢失。

在主从保持正常连接时，主节点会异步地将自己处理的新写命令同步给从节点。**异步代表主节点的数据和从节点不是强一致的**

## 运维问题

### 读写分离

当我们将读请求分摊到从节点，只对主节点执行写操作，业务上无法避免下列问题

#### 数据延迟

前面提到主从的数据复制是异步的。所以受限于网络带宽和命令的处理速度，主从之间的数据不可能强一致。对于存在的延迟，可以考虑添加监控，根据具体的延迟情况，来做出优化调整。

首先这个外部监控程序监控的数据是**主从节点的复制偏移量之差**。如果超过设置的阈值，那么就触发报警，客户端这时候收到通知可以考虑调整从节点。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165615.png)


#### 过期数据

版本原因。Redis处理过期数据有一种策略是定期删除。也就是通过定时任务，在一段时间内进行采样，对采样到的数据中的过期键执行del命令，再将del命令同步给从节点。

在老版本中，如果遇到大量数据过期，主节点可能对这批数据还没有采样到，那么从节点收到读命令，就会返回过期数据。**但是在3.2之后，从节点会执行惰性删除，也就是读取数据之前会检查过期时间**

### 从节点故障

具体分析在哨兵一章
### 规避全量复制

前面提到了，全量复制非常耗费资源，且不稳定。所以在实际运维中，我们需要尽量规避全量复制。
- 在主节点数据量大，流量也大的情况下，添加从节点时考虑在**低峰时进行**
- **主节点故障重启之后**，运行ID发生变化，从节点发现运行Id不匹配后，会重新发起全量复制。考虑使用哨兵进行自动的故障转移。
- **复制积压缓冲区过小，**正常执行部分复制，发现缓冲区已经没有对应的数据了，不得不进行全量复制。所以需要我们通过监控，日志，结合这些数据，分析出**网络中断的时长，写命令数据量的平均值**，调整得出一个合理的缓冲区大小。
#### 规避复制风暴

回到之前的拓扑图上。如果按照普通的一主多从，那么当多个从节点要对同一个主节点，或者同一个机器上的多个主节点发起全量复制，那么主节点所在的机器需要面临巨大的开销，CPU，内存，网络带宽都有消耗。

首先Redis对于大量从节点挂载复制数据的情况是有一定的优化的。比如公用一份RDB，避免重复创建。但是无法避免向多个从节点进行网络传输。这种情况下，只有考虑使用树状拓扑，将压力分给二级主节点，**并且这些主节点不能够在同一个机器上，否则资源还是不够** 。 但不可忽略的是，这种拓扑提供了运维的复杂度

# 阻塞问题大分析

Redis是单线程架构，所有的读写操作都在一条线程上，在高并发下，一旦出现阻塞，哪怕是很短时间，都会影响到全局。（Redis作为很多服务的中间件存在）
## 发现阻塞

借助日志系统，当异常发生时，通过自定义的Appender实现触发告警的逻辑。如果使用的是Redis集群，那么在异常打印时，还需要加上节点的ip和port。
## 内因分析

### 使用API和数据结构不合理

Redis执行命令速度非常快，但是如果命令使用不合理，或者处理的对象数据结构使用不合理，都会导致慢查询。

Redis原生支持慢查询统计，通过slowlog get{n} 获取最近的n条慢查询命令。**慢查询只记录命令执行的耗时，也就是说如果某条命令的执行缓慢被记录，有可能是因为要等待其他命令的执行**
当发现慢查询后，有两个方向去做出调整。
- 替换成低时间复杂度的命令，比如hgetall改成hmget，禁用一些全库遍历的命令
- 调整大对象（大key）： 缩减大对象的数据量，或者将其拆分成多个小对象，防止一次命令操作过多的数据。具体拆分需要结合业务

> Redis 提供 redis-cli-h{ip}-p{port}-bigkeys 命令，可以扫描出大对象

### 内部CPU饱和

Redis是单线程架构，执行命令只使用一个CPU。我们需要关注CPU的使用情况，当CPU出于高使用率，不论是Redis自身使用还是其他进程占用，都会导致Redis无法处理更多的命令，面对大流量会阻塞。

通过redis-cli -stat查看redis使用情况，在这里**每秒的请求高达6w+**。意味着是大流量导致CPU占用率高。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165725.png)


我们还可以用 info commandstats，根据统计信息，分析一下某些命令的开销时间是否正常。

hset命令算法复杂度只有O（1）但平均耗时却达到135微秒，显然不合理，正常情况耗时应该在10微秒以下。这是因为上面的Redis实例为了追求低内存使用量，过度放宽ziplist使用条件。虽然采用ziplist编码后hash结构内存占用会变小，但是**操作变得更慢且更消耗CPU**

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165734.png)


### 持久化阻塞

持久化引起主线程的阻塞主要有：fork阻塞，AOF刷盘阻塞， HugePage写操作阻塞。

#### fork阻塞

fork阻塞发生在子进程复制主进程的页表，以及COW发生时。atest_fork_usec指标，表示Redis最近一次fork操作耗时。如果较大需要做出调整，比如调整实例的内存，检查当前使用的操作系统。

#### AOF刷盘阻塞

刷盘策略一般采用everySec，但是这不意味着主线程就彻底异步了。当异步线程执行fsync，因为磁盘压力过大时发生阻塞时，下一次fsync开始执行，会

#### HugePage写操作阻塞

如果操作系统开启了Transparent HugePages，那么每次cow复制的内存页单位就从4K变成了2MB，拖慢了复制的时间。

## 外因分析

### CPU竞争

前面提到Redis是CPU密集型应用，当和其他CPU密集型服务部署在一起，会影响Redis的吞吐量。即便这个CPU是多核的，但也会存在CPU的调度和上下文切换

如果一台机子的CPU是多核的，那么可以考虑将Redis进程绑定到其中一个核心。**但是在进行持久化过程，fork出的子进程会大量占用CPU，两个进程竞争一个核心，主进程吞吐量下降**

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165756.png)
### 内存交换
Redis的高性能的一个重要前提是数据都在内存中。如果机器的内存不足，那么系统会执行swap，将内存的数据转移到磁盘，需要的时候再加载回来，又回到了传统数据库的低性能了。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165809.png)

### 网络问题

#### 连接拒绝

有时候网络可能存在波动，最好的方式就是避免客户端与Redis进行异地机房调用
其次我们需要考虑到高并发情况，高并发意味着同时需要建立多个连接。
由于Redis有参数**maxclients控制客户端的最大连接数，**超过这个阈值就会拒绝新的连接。在Map/Reduce场景下，很容易出现连接数过多。因为Map/Reduce的执行逻辑就是：
- map阶段：将输入数据集分割成多个chunk，交给多个节点并行处理，这些处理的数据会存在Redis。
- Reduce阶段：从数据库拉取Map阶段的中间结果，再次处理。

客户端会频繁的创建和销毁连接，但是Redis不会主动关闭和检查闲置的TCP连接，也就是连接数快速消耗但无法释放。设置tcp-keepalive和timeout参数让Redis主动检查和关闭无效连接。

客户端和Redis的连接是建立在TCP连接上。这也会受限。首先一个连接对应一个文件句柄，操作系统会对一个进程能够建立的文件句柄做出限制。
其次TCP连接过程，是有一个backlog缓冲区，是为了提高连接建立的吞吐量，这一块也需要调整。

# 理解内存

## 内存消耗

Redis进程内的消耗主要包括：自身内存+对象内存+缓冲内存+碎片内存。自身内存也就是Redis进程本身，消耗微乎其微，可以不用考虑。

### 对象内存

对象内存占用空间最大。Redis每次创建一个键值对，就要创建一个Key对象和一个Value对象。所以对象内存的计算就是sizeof(keys) + sizeof(values). keys都是字符串，而Value包含之前提到的物种基本类型。
### 缓冲内存

Redis与客户端连接时，会分配一块缓冲区，用来缓冲客户端的输入和自己的输出。客户端也有分类。其中从节点视角的客户端，主节点会为其单独建立一条连接用于命令复制。使用发布订阅功能，订阅的客户端也会单独分到一块缓冲区，如果消费消息的速度跟不上生产消息的速度，会出现一出

在复制的一章有提到，为了实现部分复制，Redis还划分了一块复制积压缓冲区，用来临时保存最近的写命令。

在持久化一节，提到AOF重写期间的增量数据会保存到一块AOF重写缓存区中
### 碎片内存

Redis采用的内存分配器时jemalloc，它是以内存块单位分配空间的。比如保存一个5Kb的对象，Jemalloc可能分配一个8Kb内存块，剩下的3Kb就变成了随便，不能够分配了。

如果经常对数据进行更新或者过期删除，那么空间得不到释放，就会导致碎片率上升

> 当对一个键进行更新操作(如append、setrange)时,新的值可能会超出原来分配的内存块大小。这时jemalloc会重新分配一个更大的内存块来存储新的值,而原来的内存块就会被释放。
> 
> 但是,这个被释放的内存块可能无法立即被其他数据所利用。比如,原来的值是3KB,jemalloc分配了4KB的内存块。现在新的值变成了5KB,jemalloc会分配8KB的内存块,而原来的4KB内存块就被释放了。但是,由于jemalloc的内存分配策略,这个4KB的内存块可能无法满足其他数据的需求,从而变成了内存碎片。

## 内存管理

Redis可以通过config set maxmemeory来设置最大可用内存。在内存使用超过这个阈值后，会触发内存溢出控制。同时Redis也会依靠过期策略来控制内存使用。

### 删除过期对象

Redis可以为每个键值对设置过期时间，但是实际过期删除并不是精准地到点删除。因为这样会消耗大量的CPU。实际情况是采用惰性删除和定时策略。

#### 惰性删除

当客户端读取到超时的键时，再删除这个键，并且返回空。问题是如果过期间一直没有被访问，那么就永远不会被删除，对应的内存不能够得到释放。

#### 定时任务删除

内部维护一个定时任务，每10秒执行一次。会随机抽取20个键，对其中的过期键进行删除。如果发现过期键占比产过25%，就会重复这个任务逻辑，直到占比不超过25%。如果这个循环的时间超过了设定的25毫秒，那么就直接结束。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165913.png)


### 内存溢出控制

Redis提供六种策略，来决策，当内存超过阈值，如何清理数据或者摆烂。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250822165921.png)

如果选择非noeviction的策略，那么内存控制被触发时，实际上是三步走。循环查找需要删除的键，删除并回收内存，最后再执行命令。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112302.png)


## 内存优化

### 缩减键值对象

key：在能够完整描述业务情况下，尽量缩短键的长度

value：Value存储的都是业务对象。我们首先要考虑当前的业务对象是否有不必要的属性。其次业务对象可以序列化成二进制数组，配合高效的序列化工具，可以更大程度地存储数据。如果采用Json，那么可以考虑使用压缩算法，压缩后再存入。

### 共享对象池

Redis内部会维护一个[0-9999]的整数对象池，如果我们在满足需求情况下，都尽量使用整数对象，那么就能够实现复用节约空间。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112314.png)


# 哨兵机制

## 解决的问题

在复制一章中，探讨了主从架构的实现和好处。但是在实际生产环境中，需要考虑风险。所以如果主节点出现故障，那么主从架构该如何继续运转下去。在引入哨兵之前，解决方案是靠人工去执行下面操作：
- 手动修改一个从节点，将其晋升为主节点。
- 修改应用方使用的主节点地址
- 发送命令，让其他从节点去主动复制新的主节点

这个解决思路没有问题，最大的麻烦时需要人工接入。通过引入哨兵，可以实现自动故障转移。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112332.png)

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112338.png)

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112345.png)


## 安装和部署

使用redis-sentinel命令来启动Redis，具体的启动参数写在一个conf文件下。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112355.png)

哨兵只要和主节点建立连接之后，就可以获得主节点下的从节点和其他哨兵的信息。

部署尽量选择奇数个哨兵，这样在能够满足选举条件的同时，可以节省一个节点
### 拓展功能

哨兵在完成故障转移操作后，可以执行脚本，只需要在配置输入脚本的路径即可。

哨兵可以同时监控多个主节点，只需要指定不同的主节点名字。

## 客户端

客户端也就是应用方，如何在哨兵完成故障转移后，能够收到通知，切换主节点。

以 jedis 为例，我们只需要传给 jedis 一个哨兵集合，主节点的名字，jedis 就会执行下列的步骤
- 遍历哨兵集合，去和配置的主节点做比对，判断是否一致。一致则代表找到了一个可用的哨兵。找不到则抛出异常。
- 找到哨兵，也就找到了主节点，也就是找到了其他的哨兵。
- 客户端给每一个哨兵节点创建一个线程，订阅哨兵的 switch-master 频道。哨兵会在故障转移后在这个频道上发布消息。


## 实现原理

### 三个监控
一个哨兵节点通过三个定时监控任务实现节点发现和监控
#### 发现节点
每隔十秒，向主节点和从节点发送 info 命令，获取拓扑结构。当有新的从节点加入可以感知到。
#### 哨兵之间同步
每隔两秒，在自己监控下的节点的 Sentinel-hello频道发布自己对主节点状态的判断，以及自己的节点信息。
#### 对其他节点监控
每个一秒，哨兵节点会向其他节点发送ping命令，做心跳检测，确认这些节点是否可达。这个任务是判断节点异常的重要依据

### 主观下线
哨兵节点每秒的心跳检测，如果对应的节点在规定时间内没有有效回复，那么当前哨兵节点就会主观认为这个节点已经挂掉了。

![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112440.png)

### 客观下线
当哨兵节点认为的主观下线节点是主节点时，就会询问其他的哨兵节点，他们对主节点状态的判断。只要这个值超过设置的“quorum”， 那么这个哨兵节点就可以做出客观下线的决定，也就是开始故障转移。
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112504.png)

> 这里和其他哨兵节点沟通是通过sentinel is-master-down-by-addr命令。这里会传一个*，代表响应方的返回结果代表他们对当前主节点的是否可达。如果是一个具体的运行ID，也就是自己的，响应方的返回结果代表是否同意自己当领导者

### 节点选举
在确定要对主节点进行故障转移后，首先要选一个哨兵节点来执行具体的流程。这里的选举思路如下：
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112543.png)
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112555.png)
![image.png](https://obsidian-1317277327.cos.ap-chengdu.myqcloud.com/attachment/20250825112614.png)

**基本上谁先完成客观下线，谁就是领导者。**
### 故障转移
- 在从节点列表中选择一个节点作为新的节点，选择标准是：
    - 过滤掉主观下线的，与主节点失联过且较长时间，5秒内没有回复过哨兵节点的ping。
    - 选择slave-priority参数的值最大的
    - 选择复制偏移量最大的。
    - 选择runid最小的。
- 哨兵对选出的从节点执行slave no one命令，晋升为主节点
- 哨兵发送命令，让其他的从节点切换主节点。
- 将原先的主节点更新完从节点，并且监控，当他恢复后去复制新的主节点。
## 运维问题

### 节点下线
节点下线分为
**临时下线**：暂时关闭节点，后续会重新启动
**永久下线：** 将节点关掉后不再使用，需要进行一些清理工作。
对主节点下线过程是，选择一个比较优秀的从节点，哨兵直接执行failover，强制开启故障转移。如果一个主节点存在多个从节点，那我们可以把剩余节点的salve priority设置为0；
对于从节点，我们需要考虑读写分离情况下，确保应用方知道节点下线的变化，从而将读请求路由到其他的节点。
### 节点上线
上线一个新的从节点，无非是为了解决几个问题：
- 现有的从节点已经无法支撑应用方的流量
- 主节点没有可用的从节点来支持故障转移
- 需要用一个性能更好的新的从节点来替换原先的主节点

只需要配置好从节点的slave信息，然后启动即可。哨兵会感知到从节点的上线。
上线一个哨兵节点，情况是：
- 当前的哨兵节点不够，导致系统健壮性不足，或者无法投票选举
- 去替换原先的哨兵节点。
也是只需要添加主节点的配置，然后启动，等待其他哨兵发现。[]()
添加主节点比较简单，因为哨兵只认一个主节点，直接手动执行sentinel failover发起故障转移即可